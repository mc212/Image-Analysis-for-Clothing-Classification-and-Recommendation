{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P_polyvore_data_preparation2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "79QIwZJLJkTx",
        "SgjxsMGZoN4P",
        "k8UYHCJIqVrm",
        "Sk2BFoNi0igl",
        "qnImtbLn1-ED",
        "9erzEzc2BjW0"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyizCYkY-e_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b2c3d2-1fc0-4f28-b92b-109ba210de6e"
      },
      "source": [
        "\"\"\"\"\n",
        "After annotating the clothing images using VGG16/ Resnet50/ InceptionV3 based- models, the received data are the input for learning the compatibility.\n",
        "\n",
        "This is the preparation for compatibility learning using BPR model, applying Polyvore's annotation and self-mapped category of clothing item attributes and which part does that attribute take part on body (top/ bottom).\n",
        "To be more specific, the work below are:\n",
        "- Transform the labeled data \n",
        "-> Build the BPR model to learn the pair score (between top-bottom clothing item attributes) \n",
        "-> Make the recommendation based on the pair scores\n",
        "\n",
        "\"\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/Practicum- ChauDao')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFRh9so0-7yh"
      },
      "source": [
        "df_train = pd.read_json (r'./Polyvore/train_no_dup_new_100.json')\n",
        "df_test = pd.read_json (r'./Polyvore/test_no_dup_new_100.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvwSP0TFEBkn"
      },
      "source": [
        "category_list_tb = pd.read_csv (r'./Polyvore/Filtered_top_bottom/category_summarize_top_bottom.csv')\n",
        "#category_summarize_top_bottom.csv: manually mapping the original category of Polyvore dataset to top/bottom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx1GcwiJApW-"
      },
      "source": [
        "##1. Remove cats not top/bottom\n",
        "output: df_train_tb_fin, df_test_tb_fin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlhxSTQOL3SD"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuxJf9c-_Ljx"
      },
      "source": [
        "#append df.items_category: remove item that not declare in category_list_tb\n",
        "#df_train\n",
        "df_train_tb = pd.DataFrame(columns=['set_id', 'items_category','new_items_category'])\n",
        "cat = list(category_list_tb[\"id\"])\n",
        "data = []\n",
        "\n",
        "for set_id in tqdm(df_train.set_id.unique()):\n",
        "  items_category = df_train[(df_train.set_id == set_id)].items_category.values\n",
        "  \n",
        "  old = items_category[0][:]\n",
        "  new_items_category = []\n",
        "  \n",
        "  for i in range(len(old)):\n",
        "    if old[i] in cat:\n",
        "      new_items_category.append(old[i])\n",
        "\n",
        "  data.append({'set_id': set_id, 'items_category': items_category[0][:], 'new_items_category': new_items_category})\n",
        "\n",
        "df_train_tb = df_train_tb.append(data, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfxQeDJCL-KD"
      },
      "source": [
        "#append df.items_category: remove item that not declare in category_list_tb\n",
        "#df_test\n",
        "df_test_tb = pd.DataFrame(columns=['set_id', 'items_category','new_items_category'])\n",
        "cat = list(category_list_tb[\"id\"])\n",
        "data = []\n",
        "\n",
        "for set_id in tqdm(df_test.set_id.unique()):\n",
        "  items_category = df_test[(df_test.set_id == set_id)].items_category.values\n",
        "  \n",
        "  old = items_category[0][:]\n",
        "  new_items_category = []\n",
        "  \n",
        "  for i in range(len(old)):\n",
        "    if old[i] in cat:\n",
        "      new_items_category.append(old[i])\n",
        "\n",
        "  data.append({'set_id': set_id, 'items_category': items_category[0][:], 'new_items_category': new_items_category})\n",
        "\n",
        "df_test_tb = df_test_tb.append(data, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7H96ip1MH4r"
      },
      "source": [
        "df_test_tb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4v9gkY6McjL"
      },
      "source": [
        "df_train_tb_fin = pd.DataFrame(columns=['set_id', 'items_category','new_items_category'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km7uD9w2ML7z"
      },
      "source": [
        "##Remove set_id having number of items <=1\n",
        "#train\n",
        "data = []\n",
        "for set_id in tqdm(df_train_tb.set_id.unique()):\n",
        "  items_category = df_train_tb[(df_train_tb.set_id == set_id)].items_category.values\n",
        "  new_items_category = df_train_tb[(df_train_tb.set_id == set_id)].new_items_category.values\n",
        "  new_ic = new_items_category[0][:]\n",
        "  if len(new_ic) >1:\n",
        "    data.append({'set_id': set_id, 'items_category': items_category[0][:], 'new_items_category': new_items_category[0][:]})\n",
        "\n",
        "df_train_tb_fin = df_train_tb_fin.append(data, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nNQv5azNFfT"
      },
      "source": [
        "##Remove set_id having number of items <=1\n",
        "#test\n",
        "df_test_tb_fin = pd.DataFrame(columns=['set_id', 'items_category','new_items_category'])\n",
        "\n",
        "data = []\n",
        "for set_id in tqdm(df_test_tb.set_id.unique()):\n",
        "  items_category = df_test_tb[(df_test_tb.set_id == set_id)].items_category.values\n",
        "  new_items_category = df_test_tb[(df_test_tb.set_id == set_id)].new_items_category.values\n",
        "  new_ic = new_items_category[0][:]\n",
        "  if len(new_ic) >1:\n",
        "    data.append({'set_id': set_id, 'items_category': items_category[0][:], 'new_items_category': new_items_category[0][:]})\n",
        "\n",
        "df_test_tb_fin = df_test_tb_fin.append(data, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My2S0JOKO3l6"
      },
      "source": [
        "df_train_tb_fin.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1vNlP5IRBVD"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czRx-tv-REhJ"
      },
      "source": [
        "df_train_tb_fin.to_pickle(\"./Polyvore/Filtered_top_bottom/df_train_tb_fin.pkl\")\n",
        "df_test_tb_fin.to_pickle(\"./Polyvore/Filtered_top_bottom/df_test_tb_fin.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTNOfxxpAukI"
      },
      "source": [
        "##2. Edit df => structure: top, bottom in pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9cCU4EROc7I"
      },
      "source": [
        "###view number of set include item in group 3 (top1_bottom2_full3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE-S6Yj_OMIy"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from textblob import TextBlob\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtpv2M2UO0mC"
      },
      "source": [
        "df1 = pd.DataFrame() \n",
        "result = [] \n",
        "\n",
        "for set_id in tqdm(df_train_tb_fin.set_id.unique()):\n",
        "  list1 = df_train_tb_fin.loc[df_train_tb_fin['set_id'] == set_id, 'new_items_category'].values[0]\n",
        "  str1 = ' '.join(str(e) for e in list1)\n",
        "  result.append({\"set_id\": set_id,\"str\": str1})\n",
        "df1 = df1.append(result, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjMur-z5Obma"
      },
      "source": [
        "def get_top_n_bigram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(0, 1)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRznUEj3Oioa"
      },
      "source": [
        "common_words = get_top_n_bigram(df1['str'], 41)\n",
        "for word, freq in common_words:\n",
        "    print(word, freq)\n",
        "df3 = pd.DataFrame(common_words, columns = ['pair' , 'count'])\n",
        "df3.groupby('pair').sum()['count'].sort_values(ascending=False).plot(\n",
        "    kind='bar', ylabel='Count', title='Top 20 unigrams')\n",
        "## dont have set id with item_category in (3,4,5): group 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ErLUkfKQM29"
      },
      "source": [
        "###split new_items_category into 2 new columns: top, bottom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnfgajihNtY0"
      },
      "source": [
        "df_test_tb_fin.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFXVOH0WOLNr"
      },
      "source": [
        "top = category_list_tb.loc[category_list_tb['top1_bottom2_full3'] == 1, ('id','name','top1_bottom2_full3')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzDf5v1MSdsR"
      },
      "source": [
        "bottom = category_list_tb.loc[category_list_tb['top1_bottom2_full3'] == 2, ('id','name','top1_bottom2_full3')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvWb851dYJhX"
      },
      "source": [
        "df_train_tb_fin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyEyS9w3S8sI"
      },
      "source": [
        "df_train = pd.DataFrame(columns=['set_id', 'new_items_category','top','bottom'])\n",
        "cat_top = list(top[\"id\"])\n",
        "cat_bottom = list(bottom[\"id\"])\n",
        "data = []\n",
        "\n",
        "for set_id in tqdm(df_train_tb_fin.set_id.unique()):\n",
        "  items_category = df_train_tb_fin[(df_train_tb_fin.set_id == set_id)].new_items_category.values\n",
        "  \n",
        "  old = items_category[0][:]\n",
        "  top_list = []\n",
        "  bottom_list = []\n",
        "  \n",
        "  for i in range(len(old)):\n",
        "    if old[i] in cat_top:\n",
        "      top_list.append(old[i])\n",
        "    if old[i] in cat_bottom:\n",
        "      bottom_list.append(old[i])\n",
        "\n",
        "  if len(top_list)>=1 and len(bottom_list)>=1:\n",
        "    data.append({'set_id': set_id, 'new_items_category': items_category[0][:], 'top': top_list,'bottom': bottom_list})\n",
        "\n",
        "df_train = df_train.append(data, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSvQOUeAVbV_"
      },
      "source": [
        "df_train#9948 rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_MJJIMOYvst"
      },
      "source": [
        "df_test = pd.DataFrame(columns=['set_id', 'new_items_category','top','bottom'])\n",
        "cat_top = list(top[\"id\"])\n",
        "cat_bottom = list(bottom[\"id\"])\n",
        "data = []\n",
        "\n",
        "for set_id in tqdm(df_test_tb_fin.set_id.unique()):\n",
        "  items_category = df_test_tb_fin[(df_test_tb_fin.set_id == set_id)].new_items_category.values\n",
        "  \n",
        "  old = items_category[0][:]\n",
        "  top_list = []\n",
        "  bottom_list = []\n",
        "  \n",
        "  for i in range(len(old)):\n",
        "    if old[i] in cat_top:\n",
        "      top_list.append(old[i])\n",
        "    if old[i] in cat_bottom:\n",
        "      bottom_list.append(old[i])\n",
        "\n",
        "  if len(top_list)>=1 and len(bottom_list)>=1:\n",
        "    data.append({'set_id': set_id, 'new_items_category': items_category[0][:], 'top': top_list,'bottom': bottom_list})\n",
        "\n",
        "df_test = df_test.append(data, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "251yWJlKY4ap"
      },
      "source": [
        "df_test#1211 rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzBU6Wf1_oZR"
      },
      "source": [
        "df_train.to_pickle(\"./Polyvore/Filtered_top_bottom/train/df_train.pkl\")\n",
        "df_test.to_pickle(\"./Polyvore/Filtered_top_bottom/test/df_test.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hqeUY8M-97t"
      },
      "source": [
        "### split each top to each bottom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZwSyVRzBPkj"
      },
      "source": [
        "from tqdm import tqdm "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYEV3ES2_Fkk"
      },
      "source": [
        "df_train = pd.read_pickle ('./Polyvore/Filtered_top_bottom/train/df_train.pkl')\n",
        "df_test = pd.read_pickle ('./Polyvore/Filtered_top_bottom/test/df_test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3QXqe8wC-sq"
      },
      "source": [
        "import itertools  \n",
        "\n",
        "#train \n",
        "df_train_split = pd.DataFrame() \n",
        "\n",
        "for set_id in tqdm(df_train.set_id.unique()):\n",
        "  #x = df[df['set_id'] == set_id].index\n",
        "  A = df_train.loc[df_train['set_id'] == set_id, 'top'].values[0]\n",
        "  B = df_train.loc[df_train['set_id'] == set_id, 'bottom'].values[0]\n",
        "  c = list(itertools.product(A, B))\n",
        "  temp = pd.DataFrame(c,columns =['top', 'bottom'])\n",
        "  temp['set_id'] = set_id\n",
        "  df_train_split = df_train_split.append(temp)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssvHemKVDwC_"
      },
      "source": [
        "df_train_split.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKn5n5xNEbsi"
      },
      "source": [
        "#test\n",
        "df_test_split = pd.DataFrame() \n",
        "\n",
        "for set_id in tqdm(df_test.set_id.unique()):\n",
        "  #x = df[df['set_id'] == set_id].index\n",
        "  A = df_test.loc[df_test['set_id'] == set_id, 'top'].values[0]\n",
        "  B = df_test.loc[df_test['set_id'] == set_id, 'bottom'].values[0]\n",
        "  c = list(itertools.product(A, B))\n",
        "  temp = pd.DataFrame(c,columns =['top', 'bottom'])\n",
        "  temp['set_id'] = set_id\n",
        "  df_test_split = df_test_split.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfZKKgOLEz84"
      },
      "source": [
        "### update to new category list (range(0:n))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAIx_3L6Fx3B"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlbgdxXZEyyI"
      },
      "source": [
        "sorted(df_train_split.top.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ndp1N-5HBTz"
      },
      "source": [
        "category_list_tb = pd.read_csv (r'./Polyvore/Filtered_top_bottom/category_summarize_top_bottom.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_do0IT2HDQT"
      },
      "source": [
        "category_list_tb.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yLYcx4NFeyB"
      },
      "source": [
        "unique_top = df_train_split.top.unique()\n",
        "top_ids = dict(zip(unique_top, np.arange(unique_top.shape[0], dtype=np.int32)))\n",
        "\n",
        "unique_bottom = df_train_split.bottom.unique()\n",
        "bottom_ids = dict(zip(unique_bottom, np.arange(unique_bottom.shape[0], dtype=np.int32)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK3QPxaWeAHY"
      },
      "source": [
        "bottom_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvNPC5wkdQMc"
      },
      "source": [
        "bottom = pd.DataFrame(bottom_ids.items(), columns=['old', 'new'])\n",
        "top = pd.DataFrame(top_ids.items(), columns=['old', 'new'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8gAHShfe7vC"
      },
      "source": [
        "top.to_pickle(\"./Polyvore/Filtered_top_bottom/top_categories.pkl\")\n",
        "bottom.to_pickle(\"./Polyvore/Filtered_top_bottom/bottom_categories.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJt651OIfAc2"
      },
      "source": [
        "top"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxCBs6gbF4qI"
      },
      "source": [
        "df_train_split['top_id'] = df_train_split.top.apply(lambda u: top_ids[u])\n",
        "df_train_split['bottom_id'] = df_train_split.bottom.apply(lambda m: bottom_ids[m])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CubyK4RSGDIE"
      },
      "source": [
        "df_train_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aesagt5hGK94"
      },
      "source": [
        "sorted(df_test_split.top.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHhpl_HBGQS3"
      },
      "source": [
        "df_test_split['top_id'] = df_test_split.top.apply(lambda u: top_ids[u])\n",
        "df_test_split['bottom_id'] = df_test_split.bottom.apply(lambda m: bottom_ids[m])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jADy7mNKIILp"
      },
      "source": [
        "df_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC2TzO8oI7i4"
      },
      "source": [
        "df_train_split.to_pickle(\"./Polyvore/Filtered_top_bottom/train/df_train_split_map.pkl\")\n",
        "df_test_split.to_pickle(\"./Polyvore/Filtered_top_bottom/test/df_test_split_map.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79QIwZJLJkTx"
      },
      "source": [
        "## 3. Create file df_triplest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSgUA_KNJs0X"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dft9w9wLnP3"
      },
      "source": [
        "df_train_triplest = pd.DataFrame(columns=['set_id', 'positive_item_id', 'negative_item_id'])\n",
        "# set_id = top_id; positive_item_id = positive_bottom_id, negative_item_id = negative_item_id\n",
        "\n",
        "cat = list(bottom[\"new\"]) #np.arange(0,14, 1)\n",
        "data = []\n",
        "\n",
        "for top_id in tqdm(df_train_split.top_id.unique()):\n",
        "  positive_items = df_train_split[(df_train_split.top_id == top_id)].bottom_id.unique()\n",
        "\n",
        "  pos = positive_items[:]\n",
        "  negative_items = cat\n",
        "  for i in range(len(pos)):\n",
        "      element = pos[i]\n",
        "      c = negative_items\n",
        "      negative_items = [c[k] for k in range(len(c)) if c[k] != element ]\n",
        "\n",
        "  data.append({'set_id': top_id, 'positive_item_id': positive_items[:], 'negative_item_id': negative_items})\n",
        "df_train_triplest = df_train_triplest.append(data, ignore_index=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0YFKaVcNafO"
      },
      "source": [
        "df_train_triplest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gQE27bHP-ms"
      },
      "source": [
        "df_test_triplest = pd.DataFrame(columns=['set_id', 'positive_item_id', 'negative_item_id'])\n",
        "# set_id = top_id; positive_item_id = positive_bottom_id, negative_item_id = negative_item_id\n",
        "\n",
        "cat = list(bottom[\"new\"]) #np.arange(0,14, 1)\n",
        "data = []\n",
        "\n",
        "for top_id in tqdm(df_test_split.top_id.unique()):\n",
        "  positive_items = df_test_split[(df_test_split.top_id == top_id)].bottom_id.unique()\n",
        "\n",
        "  pos = positive_items[:]\n",
        "  negative_items = cat\n",
        "  for i in range(len(pos)):\n",
        "      element = pos[i]\n",
        "      c = negative_items\n",
        "      negative_items = [c[k] for k in range(len(c)) if c[k] != element ]\n",
        "\n",
        "  data.append({'set_id': top_id, 'positive_item_id': positive_items[:], 'negative_item_id': negative_items})\n",
        "df_test_triplest = df_test_triplest.append(data, ignore_index=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-tljm30QJud"
      },
      "source": [
        "df_test_triplest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTQSetkDgQDB"
      },
      "source": [
        "df_train_triplest.to_pickle(\"./Polyvore/Filtered_top_bottom/train/df_train_triplest.pkl\")\n",
        "df_test_triplest.to_pickle(\"./Polyvore/Filtered_top_bottom/test/df_test_triplest.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7VqcP0pQUTl"
      },
      "source": [
        "import itertools  \n",
        "\n",
        "df_train_triplest_fin = pd.DataFrame() \n",
        "\n",
        "for set_id in tqdm(df_train_triplest.set_id.unique()):\n",
        "  #x = df[df['set_id'] == set_id].index\n",
        "  A = df_train_triplest.loc[df_train_triplest['set_id'] == set_id, 'positive_item_id'].values[0]\n",
        "  B = df_train_triplest.loc[df_train_triplest['set_id'] == set_id, 'negative_item_id'].values[0]\n",
        "  if len(B)!= 0:\n",
        "    c = list(itertools.product(A, B))\n",
        "    temp = pd.DataFrame(c,columns =['positive_item_id', 'negative_item_id'])\n",
        "    temp['set_id'] = set_id\n",
        "  df_train_triplest_fin = df_train_triplest_fin.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQPHls_0Qkq4"
      },
      "source": [
        "df_train_triplest_fin.head(5) #4323 rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgzKnz4UQrDS"
      },
      "source": [
        "#import itertools  \n",
        "\n",
        "df_test_triplest_fin = pd.DataFrame() \n",
        "\n",
        "for set_id in tqdm(df_test_triplest.set_id.unique()):\n",
        "  #x = df[df['set_id'] == set_id].index\n",
        "  A = df_test_triplest.loc[df_test_triplest['set_id'] == set_id, 'positive_item_id'].values[0]\n",
        "  B = df_test_triplest.loc[df_test_triplest['set_id'] == set_id, 'negative_item_id'].values[0]\n",
        "  c = list(itertools.product(A, B))\n",
        "  temp = pd.DataFrame(c,columns =['positive_item_id', 'negative_item_id'])\n",
        "  temp['set_id'] = set_id\n",
        "  df_test_triplest_fin = df_test_triplest_fin.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Int7-ZQzmc"
      },
      "source": [
        "df_test_triplest_fin.head(5)#3776 rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri7gVug3Q2y8"
      },
      "source": [
        "df_train_triplest_fin.to_pickle(\"./Polyvore/Filtered_top_bottom/train/df_train_triplest_fin.pkl\")\n",
        "df_test_triplest_fin.to_pickle(\"./Polyvore/Filtered_top_bottom/test/df_test_triplest_fin.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmhbQJY6TGaX"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNjhwQsDc-K0"
      },
      "source": [
        "top= pd.read_pickle(\"./Polyvore/Filtered_top_bottom/top_categories.pkl\")\n",
        "bottom= pd.read_pickle(\"./Polyvore/Filtered_top_bottom/bottom_categories.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAXIhZ5gXiax"
      },
      "source": [
        "df_train_split = pd.read_pickle(\"./Polyvore/Filtered_top_bottom/train/df_train_split_map.pkl\")\n",
        "df_test_split = pd.read_pickle(\"./Polyvore/Filtered_top_bottom/test/df_test_split_map.pkl\")\n",
        "df_train_triplest_fin = pd.read_pickle(\"./Polyvore/Filtered_top_bottom/train/df_train_triplest_fin.pkl\")\n",
        "df_test_triplest_fin = pd.read_pickle(\"./Polyvore/Filtered_top_bottom/test/df_test_triplest_fin.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94N66kSTS0QM"
      },
      "source": [
        "#frames = [df_train_triplest_fin, df_test_triplest_fin]\n",
        "  \n",
        "#df_triplest = pd.concat(frames)\n",
        "df_triplest = df_train_triplest_fin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crvi9TdbVSaS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "6c0ebb5e-5756-4a5f-c27f-745b68d86ec4"
      },
      "source": [
        "df_triplest.drop_duplicates()\n",
        "df_triplest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>positive_item_id</th>\n",
              "      <th>negative_item_id</th>\n",
              "      <th>set_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>10</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>12</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>12</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>376 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    positive_item_id  negative_item_id  set_id\n",
              "0                  0                12       0\n",
              "1                  1                12       0\n",
              "2                  2                12       0\n",
              "3                  3                12       0\n",
              "4                  8                12       0\n",
              "..               ...               ...     ...\n",
              "21                10                14      15\n",
              "22                 5                13      15\n",
              "23                 5                14      15\n",
              "24                12                13      15\n",
              "25                12                14      15\n",
              "\n",
              "[376 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRnfjucZURMT"
      },
      "source": [
        "frames1 = [df_train_split, df_test_split]\n",
        "  \n",
        "df_full = pd.concat(frames1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPOEFYEmW_V1"
      },
      "source": [
        "df_full.drop_duplicates()\n",
        "df_full"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRwpKlKFUA07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e953ab2e-7fbe-49cc-e83b-8eeb1e535eed"
      },
      "source": [
        "unique_users = top.new.unique()\n",
        "unique_movies = bottom.new.unique() #np.arange(0,34, 1) 33 category\n",
        "\n",
        "num_users = unique_users.shape[0]\n",
        "num_items = unique_movies.shape[0]\n",
        "\n",
        "num_users, num_items"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjCybilPjvDp"
      },
      "source": [
        "unique_movie_ids = list(bottom.new.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-doAEFbTmk8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import path\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "from typing import Dict\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Input, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8jB5To4Trtr"
      },
      "source": [
        "def bpr_predict(model: Model, user_id: int, item_ids: list, user_layer='user_embedding', item_layer='item_embedding'):\n",
        "    \"\"\"\n",
        "    Predict by multiplication user vector by item matrix\n",
        "    \n",
        "    :return: list of the scores\n",
        "    \"\"\"\n",
        "    user_vector = model.get_layer(user_layer).get_weights()[0][user_id]\n",
        "    item_matrix = model.get_layer(item_layer).get_weights()[0][item_ids]\n",
        "\n",
        "    scores = (np.dot(user_vector, item_matrix.T))\n",
        "\n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR-6YnxZTtb7"
      },
      "source": [
        "@tf.function\n",
        "def identity_loss(_, y_pred):\n",
        "    return tf.math.reduce_mean(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvfffUtUTv_E"
      },
      "source": [
        "@tf.function\n",
        "def bpr_triplet_loss(X: dict):\n",
        "    \"\"\"\n",
        "    Calculate triplet loss - as higher the difference between positive interactions\n",
        "    and negative interactions as better\n",
        "\n",
        "    :param X: X contains the user input, positive item input, negative item input\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    positive_item_latent, negative_item_latent, user_latent = X\n",
        "\n",
        "    positive_interactions = tf.math.reduce_sum(tf.math.multiply(user_latent, positive_item_latent), axis=-1, keepdims=True)\n",
        "    negative_interactions = tf.math.reduce_sum(tf.math.multiply(user_latent, negative_item_latent), axis=-1, keepdims=True)\n",
        "\n",
        "    return tf.math.subtract(tf.constant(1.0), tf.sigmoid(tf.math.subtract(positive_interactions, negative_interactions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmOAT5u7Tyry"
      },
      "source": [
        "def out_shape(shapes):\n",
        "    return shapes[0]\n",
        "    \n",
        "\n",
        "def build_model(num_users: int, num_items: int, latent_dim: int) -> Model:\n",
        "    \"\"\"\n",
        "    Build a model for Bayesian personalized ranking\n",
        "\n",
        "    :param num_users: a number of the unique users\n",
        "    :param num_items: a number of the unique movies\n",
        "    :param latent_dim: vector length for the latent representation\n",
        "    :return: Model\n",
        "    \"\"\"\n",
        "    user_input = Input((1,), name='user_input')\n",
        "\n",
        "    positive_item_input = Input((1,), name='positive_item_input')\n",
        "    negative_item_input = Input((1,), name='negative_item_input')\n",
        "    # One embedding layer is shared between positive and negative items\n",
        "    item_embedding_layer = Embedding(num_items, latent_dim, name='item_embedding', input_length=1)\n",
        "\n",
        "    positive_item_embedding = Flatten()(item_embedding_layer(positive_item_input))\n",
        "    negative_item_embedding = Flatten()(item_embedding_layer(negative_item_input))\n",
        "\n",
        "    user_embedding = Embedding(num_users, latent_dim, name='user_embedding', input_length=1)(user_input)\n",
        "    user_embedding = Flatten()(user_embedding)\n",
        "\n",
        "    triplet_loss = Lambda(bpr_triplet_loss, output_shape=out_shape)([positive_item_embedding,\n",
        "                                                             negative_item_embedding,\n",
        "                                                             user_embedding])\n",
        "\n",
        "    model = Model(inputs=[positive_item_input, negative_item_input, user_input], outputs=triplet_loss)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN-9-Y-bT30T"
      },
      "source": [
        "'''\n",
        "#1\n",
        "latent_dim = 32\n",
        "batch_size = 125 \n",
        "num_epochs = 1\n",
        "lr = 0.001\n",
        "\n",
        "#2\n",
        "latent_dim = 32\n",
        "batch_size = 125 \n",
        "num_epochs = 5 ##\n",
        "lr = 0.001\n",
        "\n",
        "\n",
        "#3\n",
        "latent_dim = 32\n",
        "batch_size = 125 \n",
        "num_epochs = 1\n",
        "lr = 0.01 ##\n",
        "\n",
        "#4\n",
        "latent_dim = 32\n",
        "batch_size = 256 ##\n",
        "num_epochs = 1\n",
        "lr = 0.001\n",
        "\n",
        "#5\n",
        "latent_dim = 64 ##\n",
        "batch_size = 256 \n",
        "num_epochs = 1\n",
        "lr = 0.001\n",
        "\n",
        "#6\n",
        "latent_dim = 32\n",
        "batch_size = 125 \n",
        "num_epochs = 10 ##\n",
        "lr = 0.001\n",
        "\n",
        "#7\n",
        "latent_dim = 32\n",
        "batch_size = 125 \n",
        "num_epochs = 5\n",
        "lr = 0.01\n",
        "'''\n",
        "#8\n",
        "latent_dim = 32\n",
        "batch_size = 125 \n",
        "num_epochs = 1\n",
        "lr = 0.1\n",
        "'''\n",
        "'''\n",
        "model = build_model(num_users, num_items, latent_dim)\n",
        "model.compile(loss=identity_loss, optimizer=Adam(learning_rate=lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ss0zGFtU0xd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb443e4-0024-4b55-f999-9bd31c6a733a"
      },
      "source": [
        "trainable_count = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
        "non_trainable_count = np.sum([K.count_params(w) for w in model.non_trainable_weights])\n",
        "\n",
        "print('Total number of parameters: {:,}'.format(trainable_count + non_trainable_count))\n",
        "print('Trainable number of parameters: {:,}'.format(trainable_count))\n",
        "print('Non-trainable number of parameters: {:,}'.format(non_trainable_count))\n",
        "\n",
        "print('Training data length: {:,}'.format(df_triplest.shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of parameters: 992.0\n",
            "Trainable number of parameters: 992\n",
            "Non-trainable number of parameters: 0.0\n",
            "Training data length: 376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D__h4Gw9i_-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc67b182-122f-483e-9028-16ee5143c8fc"
      },
      "source": [
        "# fix error Failed to convert a NumPy array to a Tensor (Unsupported object type int)\n",
        "df_triplest['negative_item_id'] = np.asarray(df_triplest.negative_item_id).astype(np.int64)\n",
        "df_triplest['positive_item_id'] = np.asarray(df_triplest.positive_item_id).astype(np.int64)\n",
        "\n",
        "df_triplest.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive_item_id    int64\n",
              "negative_item_id    int64\n",
              "set_id              int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o_J_hhyU5Mi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28077947-28c9-45ba-eab1-d50d09f8bed9"
      },
      "source": [
        "#%%time\n",
        "\n",
        "X = {\n",
        "    'user_input': tf.convert_to_tensor(df_triplest.set_id),\n",
        "    'positive_item_input': tf.convert_to_tensor(df_triplest.positive_item_id),\n",
        "    'negative_item_input': tf.convert_to_tensor(df_triplest.negative_item_id)\n",
        "    \n",
        "}\n",
        "\n",
        "model.fit(X,tf.ones(df_triplest.shape[0]), batch_size=batch_size,epochs=num_epochs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f40a1b9ba10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjEDnXY8oICP"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgjxsMGZoN4P"
      },
      "source": [
        "### Def function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4HgHUXuoGDJ"
      },
      "source": [
        "def full_auc(model: Model, ground_truth: Dict[int, list], items: list) -> float:\n",
        "    \"\"\"\n",
        "    Measure AUC for model and ground truth for all items\n",
        "    :param model: \n",
        "    :param ground_truth: dictionary of the users and the high ranked movies for the specific user\n",
        "    :param items: a list of the all available movies\n",
        "    :return: AUC\n",
        "    \"\"\"\n",
        "\n",
        "    number_of_items = len(items)\n",
        "    scores = []\n",
        "\n",
        "    for user_id, true_item_ids in ground_truth:\n",
        "        predictions = bpr_predict(model, user_id, items)\n",
        "        grnd = np.zeros(number_of_items, dtype=np.int32)\n",
        "\n",
        "        for p in true_item_ids:\n",
        "            index = items.index(p)\n",
        "            grnd[index] = 1\n",
        "\n",
        "        if true_item_ids:\n",
        "          try:\n",
        "            a = roc_auc_score(grnd, predictions)\n",
        "          except ValueError:\n",
        "            pass\n",
        "          scores.append(a)\n",
        "\n",
        "    return sum(scores) / len(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZZrxrXjoQaP"
      },
      "source": [
        "def mean_average_precision_k(model: Model, \n",
        "                           ground_truth: Dict[int, list], \n",
        "                           items: list, \n",
        "                           k=100) -> float:\n",
        "    \"\"\"\n",
        "    Calculate mean eavarage precission per user\n",
        "    \n",
        "    :param model: \n",
        "    :param ground_truth: dictionary of the users and the high ranked movies for the specific user\n",
        "    :param items: a list of the all available movies\n",
        "    :param k: top N recommendations per user\n",
        "    :return: mean eavarage precission\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "\n",
        "    for user, actual in ground_truth:\n",
        "        predictions = bpr_predict(model, user, items)\n",
        "        predictions = dict(zip(items, predictions))\n",
        "        predictions = sorted(predictions.items(), key=lambda kv: kv[1], reverse=True)[:k]\n",
        "        predictions = list(OrderedDict(predictions).keys())\n",
        "\n",
        "        score = 0.0\n",
        "        num_hits = 0.0\n",
        "\n",
        "        for i, p in enumerate(predictions):\n",
        "            if p in actual:\n",
        "                num_hits += 1.0\n",
        "                score += num_hits / (i + 1.0)\n",
        "\n",
        "        score = score / min(len(actual), k)\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8UYHCJIqVrm"
      },
      "source": [
        "### prepare ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIvSkXTHbK1M"
      },
      "source": [
        "df_train_split = pd.read_pickle(\"./Polyvore/Filtered_top_bottom/train/df_train_split_map.pkl\")\n",
        "df_test_split = pd.read_pickle(\"./Polyvore/Filtered_top_bottom/test/df_test_split_map.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKvDeHbxq2R6"
      },
      "source": [
        "df_train_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxGvlKN9yttz"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZXovLUOzJAy"
      },
      "source": [
        "# train\n",
        "ground_truth_train = pd.DataFrame(columns=['top_id', 'bottom_id'])\n",
        "data = []\n",
        "for top_id in tqdm(df_train_split.top_id.unique()):\n",
        "  b = df_train_split[(df_train_split.top_id == top_id)].bottom_id.unique()\n",
        "\n",
        "  data.append({'top_id': top_id, 'bottom_id': b})\n",
        "ground_truth_train = ground_truth_train.append(data, ignore_index=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovEJkjmPz3v4"
      },
      "source": [
        "ground_truth_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2tN3ogz1h6D"
      },
      "source": [
        "ground_truth_test = pd.DataFrame(columns=['top_id', 'bottom_id'])\n",
        "data = []\n",
        "for top_id in tqdm(df_test_split.top_id.unique()):\n",
        "  b = df_test_split[(df_test_split.top_id == top_id)].bottom_id.unique()\n",
        "\n",
        "  data.append({'top_id': top_id, 'bottom_id': b})\n",
        "ground_truth_test = ground_truth_test.append(data, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geUjKFfd10ZG"
      },
      "source": [
        "ground_truth_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVszyoTb16E5"
      },
      "source": [
        "import pickle\n",
        "\n",
        "ground_truth_train.to_pickle(\"./Polyvore/Filtered_top_bottom/train/df_ground_truth_train.pkl\")\n",
        "ground_truth_test.to_pickle(\"./Polyvore/Filtered_top_bottom/test/ground_truth_test.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOWLfj28zzSe"
      },
      "source": [
        "#Train\n",
        "##top_id\n",
        "ground_truth_train['top_id'] = np.asarray(ground_truth_train.top_id).astype(np.int64)\n",
        "##bottom_id\n",
        "for i in range(0,16):\n",
        "  a = ground_truth_train['bottom_id'][i]\n",
        "  ground_truth_train['bottom_id'][i] = a.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsq92Rivz_Nx"
      },
      "source": [
        "#Test\n",
        "##top_id\n",
        "ground_truth_test['top_id'] = np.asarray(ground_truth_test.top_id).astype(np.int64)\n",
        "##bottom_id\n",
        "for i in range(0,16):\n",
        "  a = ground_truth_test['bottom_id'][i]\n",
        "  ground_truth_test['bottom_id'][i] = a.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZywOgiZ0FyG"
      },
      "source": [
        "ground_truth_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDgW7V9W0PkP"
      },
      "source": [
        "ground_truth_test.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00i6qQRn0WOh"
      },
      "source": [
        "import pickle\n",
        "\n",
        "ground_truth_train.to_pickle(\"./Polyvore/Filtered_top_bottom/train/df_ground_truth_train.pkl\")\n",
        "ground_truth_test.to_pickle(\"./Polyvore/Filtered_top_bottom/test/ground_truth_test.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ08i6lAoSO1"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUQkTgZKjSp4"
      },
      "source": [
        "ground_truth_train = pd.read_pickle(\"./Polyvore/Filtered_top_bottom/train/df_ground_truth_train.pkl\")\n",
        "ground_truth_test = pd.read_pickle(\"./Polyvore/Filtered_top_bottom/test/ground_truth_test.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa-dT93GoXH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158354e4-5629-4871-8b9f-643dca7501dc"
      },
      "source": [
        "print(f'AUC train: {full_auc(model, ground_truth_train.values, unique_movie_ids)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC train: 0.9975961538461539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMSpr5h8oYYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4035c08-ae2d-426f-88fb-656e9a7fd620"
      },
      "source": [
        "print(f'Mean average precision train: {mean_average_precision_k(model, ground_truth_train.values, unique_movie_ids)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean average precision train: 0.9996565934065934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAbtnUL5oauU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787a9f51-4c9a-4fa8-f89b-604db32f3c62"
      },
      "source": [
        "print(f'AUC test: {full_auc(model, ground_truth_test.values, unique_movie_ids)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC test: 0.6804771501646502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyRwXHqqocI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff3cc1a-e730-49e1-ab68-848ce3eae409"
      },
      "source": [
        "print(f'Mean average precision test: {mean_average_precision_k(model, ground_truth_test.values, unique_movie_ids)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean average precision test: 0.8646615737678581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4YdGu37zG_N"
      },
      "source": [
        "## Make RCM:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36DpPlfUzpra"
      },
      "source": [
        "top= pd.read_pickle(\"./Polyvore/Filtered_top_bottom/top_categories.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSkOm2oF0eoR"
      },
      "source": [
        "### Table with full list of pair score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCMXmuZzzLQ1"
      },
      "source": [
        "#table with full list of pair score:\n",
        "\n",
        "df_pair_score =  pd.DataFrame(columns=['top_id', 'bottom_id','pair_score'])\n",
        "data = []\n",
        "a = top['new']\n",
        "\n",
        "for i in range(len(a)) :\n",
        "  top = a[i]\n",
        "  pair_score_list =  np.asarray(bpr_predict(model, top, unique_movie_ids)).astype(np.float)\n",
        "  for j in range(len(pair_score_list)):\n",
        "    index = j \n",
        "    pair_score = pair_score_list[j]\n",
        "    data.append({'top_id': top,'bottom_id':index, 'pair_score':pair_score})\n",
        "\n",
        "df_pair_score = df_pair_score.append(data,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rH0zhb3z6BV"
      },
      "source": [
        "df_pair_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk2BFoNi0igl"
      },
      "source": [
        "###Table with full bottom_list + its categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flsvVhvL0qvF"
      },
      "source": [
        "bottom_img = pd.DataFrame({'img_id': {0: '123_2', 1: '123_5', 2: '123_4', 3: '123_8', 4: '123_7', 5: '123_1', 6: '123_9', 7: '123_3', 8: '123_6'}, 'bottom_category': {0: [0,1,2], 1: [0,2,3], 2: [5,14,1], 3: [9,13,10,2,5,4], 4: [2], 5: [14,1,2,3,4], 6: [6,7], 7: [0,10], 8: [8,9,12]}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7MHWVub1eZM"
      },
      "source": [
        "bottom_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHEnh45l3ZYA"
      },
      "source": [
        "### Table with input top image's information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqfLwmx83kMq"
      },
      "source": [
        "input_top = pd.DataFrame({'img_id': {0: '234_2'},'top_category':{0: [0,2,5,9]}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQDX-OXz4BU5"
      },
      "source": [
        "input_top"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnImtbLn1-ED"
      },
      "source": [
        "### Calculate bottom_img_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBXlHdpN2sUn"
      },
      "source": [
        "top_category = input_top.top_category[0]\n",
        "data =[]\n",
        "output_bottom = pd.DataFrame(columns = ['img_id','img_score'])\n",
        "\n",
        "for img_id in bottom_img.img_id.unique():\n",
        "  bottom_category = bottom_img[(bottom_img.img_id == img_id)].bottom_category.values[0]\n",
        "  score = 0\n",
        "  img_score = 0\n",
        "  #top_category = input_top.top_category[0]\n",
        "  for i in range(0,len(bottom_category)):\n",
        "    for j in range(0,len(top_category)):\n",
        "      x = df_pair_score[(df_pair_score.top_id == top_category[j])&(df_pair_score.bottom_id == bottom_category[i])].pair_score.values\n",
        "      score = score + x\n",
        "  data.append ({'img_id':img_id,'img_score':score})\n",
        "\n",
        "output_bottom = output_bottom.append(data,ignore_index=True )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "vc6TRaHOA_ZB",
        "outputId": "889ed939-15a9-40dc-ced7-55bc7fc85f0a"
      },
      "source": [
        "output_bottom.sort_values(by=['img_score'],ascending = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_id</th>\n",
              "      <th>img_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>123_9</td>\n",
              "      <td>[0.02185550460126251]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123_2</td>\n",
              "      <td>[0.015054209623485804]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>123_5</td>\n",
              "      <td>[0.010595788015052676]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>123_3</td>\n",
              "      <td>[0.009235450124833733]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>123_4</td>\n",
              "      <td>[0.007444234448485076]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>123_7</td>\n",
              "      <td>[0.006190633401274681]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>123_1</td>\n",
              "      <td>[0.000493430532515049]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>123_6</td>\n",
              "      <td>[0.00045066187158226967]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>123_8</td>\n",
              "      <td>[-0.017440810741391033]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  img_id                 img_score\n",
              "6  123_9     [0.02185550460126251]\n",
              "0  123_2    [0.015054209623485804]\n",
              "1  123_5    [0.010595788015052676]\n",
              "7  123_3    [0.009235450124833733]\n",
              "2  123_4    [0.007444234448485076]\n",
              "4  123_7    [0.006190633401274681]\n",
              "5  123_1    [0.000493430532515049]\n",
              "8  123_6  [0.00045066187158226967]\n",
              "3  123_8   [-0.017440810741391033]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9erzEzc2BjW0"
      },
      "source": [
        "###Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoCpmOHmBSCr"
      },
      "source": [
        "def full_auc(model: Model, ground_truth: Dict[int, list], items: list) -> float:\n",
        "    \"\"\"\n",
        "    Measure AUC for model and ground truth for all items\n",
        "    \n",
        "    :param model: \n",
        "    :param ground_truth: dictionary of the users and the high ranked movies for the specific user\n",
        "    :param items: a list of the all available movies\n",
        "    :return: AUC\n",
        "    \"\"\"\n",
        "\n",
        "    number_of_items = len(items)\n",
        "    scores = []\n",
        "\n",
        "    for user_id, true_item_ids in ground_truth:\n",
        "        predictions = bpr_predict(model, user_id, items)\n",
        "        grnd = np.zeros(number_of_items, dtype=np.int32)\n",
        "\n",
        "        for p in true_item_ids:\n",
        "            index = items.index(p)\n",
        "            grnd[index] = 1\n",
        "\n",
        "        if true_item_ids:\n",
        "            scores.append(roc_auc_score(grnd, predictions))\n",
        "\n",
        "    return sum(scores) / len(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C03OLVCC_9r"
      },
      "source": [
        "def x_score(bottom_category:list, top_category:list):\n",
        "  score = 0\n",
        "  for i in range(0,len(bottom_category)):\n",
        "      for j in range(0,len(top_category)):\n",
        "        x = df_pair_score[(df_pair_score.top_id == top_category[j])&(df_pair_score.bottom_id == bottom_category[i])].pair_score.values\n",
        "        score = score + x\n",
        "  return score\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuzNDctiEmZo"
      },
      "source": [
        "def img_score(img_id ,bottom_category:list, top_category:list):\n",
        "  score_list = x_score(bottom_category,top_category)\n",
        "  img_score = tf.math.reduce_sum(score_list).numpy()\n",
        "  return img_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwWCP9THBoFF"
      },
      "source": [
        "def make_rcm(input_top):\n",
        "  top_category = input_top.top_category[0]\n",
        "  data =[]\n",
        "  output_bottom = pd.DataFrame(columns = ['img_id','img_score'])\n",
        "\n",
        "\n",
        "  for img_id in bottom_img.img_id.unique():\n",
        "    bottom_category = bottom_img[(bottom_img.img_id == img_id)].bottom_category.values[0]\n",
        "    top_category = input_top.top_category[0]\n",
        "    total_score = img_score(bottom_category,top_category)\n",
        "    data.append ({'img_id':img_id,'img_score':total_score})\n",
        "    output_bottom= output_bottom.append(data,ignore_index=True ).sort_values(by=['img_score'],ascending = False)\n",
        "    \n",
        "  return output_bottom.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "cRbcRYTkHoWf",
        "outputId": "efee0a6f-f4be-4159-c810-e5787e4caa4c"
      },
      "source": [
        "make_rcm(input_top)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-e02e207f565d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_rcm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_top\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-114-b0c73b8b56d1>\u001b[0m in \u001b[0;36mmake_rcm\u001b[0;34m(input_top)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbottom_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbottom_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mimg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottom_category\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtop_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_category\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtotal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom_category\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_category\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'img_id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'img_score'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtotal_score\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutput_bottom\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moutput_bottom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: img_score() missing 1 required positional argument: 'top_category'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "E-ZbTEHjKDF9",
        "outputId": "6151cf81-4cb8-49f3-f79c-57ffa3eb45aa"
      },
      "source": [
        "input_top"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_id</th>\n",
              "      <th>top_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>234_2</td>\n",
              "      <td>[0, 2, 5, 9]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  img_id  top_category\n",
              "0  234_2  [0, 2, 5, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    }
  ]
}